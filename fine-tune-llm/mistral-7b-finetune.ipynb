{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset='/home/recallfun/webapps/foodblog/fine-tune-llm/alpaca_finetune_dataset.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('json', data_files=train_dataset , split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    print(len(lengths))\n",
    "    print(max(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_device = Accelerator().process_index\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"/home/recallfun/llm/models/mistral-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "  device_map=\"auto\",\n",
    "  quantization_config=bnb_config)\n",
    "\n",
    "plot_data_lengths(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_lengths(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"text\"],truncation=True,\n",
    "        max_length=2000,\n",
    "        padding=\"max_length\",), batched=True)\n",
    "plot_data_lengths(train_dataset)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=64, \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"w1\",\n",
    "        \"w2\",\n",
    "        \"w3\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=2.5e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"/home/recallfun/llm/finetune/food-blog-mistral-7b-checkpoints\",\n",
    "        max_steps=1000,\n",
    "        save_total_limit = 2,\n",
    "save_steps = 250,\n",
    "load_best_model_at_end=False,\n",
    "num_train_epochs=3,\n",
    "optim=\"paged_adamw_8bit\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2000,  # You can specify the maximum sequence length here\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "peft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(\"/home/recallfun/llm/finetune/food-blog-mistral-7b-finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "text = \"You are a helpful assistant whose role is to create posts for the food blog. I will give you the dish name and you will provide the blog data in JSON format.The json format is [{ \\\"type\\\": \\\"title\\\", \\\"sequence\\\": 1, \\\"text\\\":\\\"\\\",\\\"formatting\\\":{} },{ \\\"type\\\": \\\"cuisine\\\", \\\"sequence\\\": 2, \\\"text\\\":\\\"\\\",\\\"formatting\\\":{} },{ \\\"type\\\": \\\"preparationTime\\\", \\\"sequence\\\": 3, \\\"text\\\":\\\"\\\",\\\"formatting\\\":{} },{ \\\"type\\\": \\\"cookingTime\\\", \\\"sequence\\\": 4, \\\"text\\\":\\\"\\\",\\\"formatting\\\":{} },{ \\\"type\\\": \\\"equipmentList\\\", \\\"sequence\\\": 5, \\\"text\\\": [],\\\"formatting\\\":{} },{ \\\"type\\\": \\\"ingredientList\\\", \\\"sequence\\\": 6, \\\"text\\\": [],\\\"formatting\\\":{} },{ \\\"type\\\": \\\"instructions\\\", \\\"sequence\\\": 7, \\\"text\\\": [],\\\"formatting\\\":{}}]. The ingredientList is an array of text. The instructions are also array of text.The elements of the instructions array represent the steps in ascending order. equipmentList is an array of text where each text is the name of the equipment needed to prepare the dish. Each object of the json array will have formatting object with the schema  {\\n    \\\"bold\\\": true,\\n    \\\"italic\\\": false,\\n    \\\"fontSize\\\": \\\"16px\\\",\\n    \\\"fontColor\\\": \\\"#333333\\\"\\n  }. The format object of the type title should have following values {\\\"bold\\\": true, \\\"italic\\\": false, \\\"fontSize\\\": \\\"16px\\\", \\\"fontColor\\\": \\\"#cc0000\\\"}. The format object of the type cusinie have following values {\\\"bold\\\": false, \\\"italic\\\": false, \\\"fontSize\\\": \\\"18px\\\", \\\"fontColor\\\": \\\"#ffcc00\\\"}. The format object of the type equipmentList have following values {\\\"bold\\\": false, \\\"italic\\\": true, \\\"fontSize\\\": \\\"14px\\\", \\\"fontColor\\\": \\\"#990000\\\"}. The format object of the type ingredientList have following values {\\\"bold\\\": false, \\\"italic\\\": false, \\\"fontSize\\\": \\\"14px\\\", \\\"fontColor\\\": \\\"#cc6600\\\"}.The format object of the type instructions have following values {\\\"bold\\\": false, \\\"italic\\\": true, \\\"fontSize\\\": \\\"14px\\\", \\\"fontColor\\\": \\\"#ff3300\\\"}.  The rest of the json objects will have the default value in the formatting object {\\\"bold\\\": false, \\\"italic\\\": false, \\\"fontSize\\\": \\\"16px\\\", \\\"fontColor\\\": \\\"#ff9900\\\"}.  The name of the dish is \\\"Peking Duck\\\".Your task is to add the content in the field \\\"text\\\" of every object.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
